\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath,epsfig}
\hyphenation{Tam-pe-re micro-soft}

\newcommand{\kfig}[4]{ % params: file, label, caption
        \begin{figure}[!t]
        \centering
        \includegraphics[#4]{Figures/#1}
        \vspace{-1mm}
        \caption{#3}
        \label{#2}
        \end{figure}
}



\begin{document}
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Advanced Branch Predictors for Soft Processors}


\author{\IEEEauthorblockN{Di Wu, Jorge Albericio and Andreas Moshovos}
\IEEEauthorblockA{Electrical and Computer Engineering Department\\
University of Toronto}
\IEEEauthorblockA{peterwudi.wu@utoronto.ca, jorge@eecg.toronto.edu, moshovos@eecg.toronto.edu}}

% make the title area
\maketitle


\begin{abstract}
The abstract goes here.


\end{abstract}


\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sec:intro}
Field Programmable Gate Arrays (FPGAs) are increasingly popular to be used in embedded systems. Such designs often employ one or more embedded microprocessors, and there is a trend to migrate these microprocessors to the FPGA platform. Although these soft processors cannot match the performance of a hard processor, soft processors have the advantage that the designers can implement the exact number of processors to efficiently fit the application requirements.

Current commercial soft processors such as Altera's Nios~II~\cite{niosii} and Xilinx's Microblaze~\cite{microblaze} are in-order pipelines with five to six pipeline stages. These processors are often used for less computation-intensive applications, for instance, system control tasks. To support more compute-intensive applications, a key technique to improve performance is branch prediction. Branch prediction has been extensively studies, mostly in the context of application specific custom logic (ASIC) implementations. However, na\"ively porting ASIC-based branch predictors to FPGAs results in slow and/or resource-inefficient implementations since the tradeoffs are different for reconfigurable logic. Wu et al.~\cite{grselect} have shown that a branch predictor design for soft processors should balance its prediction accuracy as well as the maximum operating frequency. They proposed an FPGA-friendly minimalistic branch prediction implementation \textit{gRselect} for Altera's highest performing soft-processor Nios~II-f.

Wu et al. limits the hardware budget of the gRselect predictor to one M9K Block RAM~\cite{StratixIVM9K} on Altera Stratix IV devices, which is the same hardware budget as Nios~II-f. Such a small hardware budget prohibits more elaborated and accurate branch prediction schemes such as perceptron~\cite{perceptron} and TAGE~\cite{tage}. This work looses the hardware budget constraint and investigates FPGA-friendly implementations of perceptron and TAGE predictors. This work also assumes a pipelined processor implementation representative of Altera's Nios~II-f for comparison versus gRselect.

Specifically, this work makes the following contributions: (1)~It studies the FPGA implementation of the perceptron predictor and TAGE predictor for branch direction prediction. It shows that comparing the accuracy versus gRselect, perceptron is ~1\% worse while TAGE is ~1\% better, assuming these predictors can be accessed in a single cycle.







\subsection{Subsection Heading Here}
Subsection text here.


\subsubsection{Subsubsection Heading Here}
Subsubsection text here.





\section{Perceptron Predictor}
\label{sec:advanced:perceptron}
Section~\ref{sec:background:dirpred:perceptron} introduced that perceptron predictor maintains vectors of weights in a table. It produces a prediction through the following steps: (1)~a vector of weight (i.e., a perceptron) is loaded from the table. (2)~multiply the weights with their corresponding global history (1 for taken and -1 for not-taken). (3)~sum up all the products, predict taken if the sum is positive, and not-taken otherwise. Each of these steps poses difficulties to map to the FPGA platform. The rest of this section addresses these problems.

\subsection{Perceptron Table Organization}
\label{sec:advanced:perceptron:table}
Each weight in a perceptron is typically 8-bit wide, and perceptron predictors usually use at least 12-bit global history~\cite{perceptron}. The depth of the table, on the other hand, tends to be relatively shallower (e.g. 64 entries for 1KB hardware budget). This requires a very wide but shallow memory, which does not map well to BRAMs on FPGAs. For example, the widest configuration that a M9K BRAM on Altera Stratix IV chip is 36-bit wide times 1k entries. If we implement the 1KB perceptron from~\cite{perceptron}, which uses 96-bit wide perceptrons 12-bit global history

As Fig.~\ref{perceptronBRAM} shows,
64-entry




\subsection{Multiplication}
\label{sec:advanced:perceptron:mult}



\subsection{Adder Tree}
\label{sec:advanced:perceptron:adder}











\section{TAgged GEometric history length Branch Predictor (TAGE)}
\label{sec:advanced:tage}









\section{Conclusion}
The conclusion goes here.




% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,bp}





% that's all folks
\end{document}


