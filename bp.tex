\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath,epsfig, url}
\usepackage[justification=centering]{caption}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = magenta %Colour of citations
}


\hyphenation{Tam-pe-re micro-soft}


\newcommand{\kfig}[4]{ % params: file, label, caption
        \begin{figure}[!t]
        \centering
        \includegraphics[#4]{Figures/#1}
        \vspace{-1mm}
        \caption{#3}
        \label{#2}
        \end{figure}
}



\begin{document}
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Advanced Branch Predictors for Soft Processors}


\author{\IEEEauthorblockN{Di Wu, Jorge Albericio and Andreas Moshovos}
\IEEEauthorblockA{Electrical and Computer Engineering Department\\
University of Toronto}
\IEEEauthorblockA{peterwudi.wu@utoronto.ca, jorge@eecg.toronto.edu, moshovos@eecg.toronto.edu}}

% make the title area
\maketitle


\begin{abstract}
The abstract goes here.


\end{abstract}


\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sec:intro}
Field Programmable Gate Arrays (FPGAs) are increasingly popular to be used in embedded systems. Such designs often employ one or more embedded microprocessors, and there is a trend to migrate these microprocessors to the FPGA platform. Although these soft processors cannot match the performance of a hard processor, soft processors have the advantage that the designers can implement the exact number of processors to efficiently fit the application requirements.

Current commercial soft processors such as Altera's Nios~II~\cite{niosii} and Xilinx's Microblaze~\cite{microblaze} are in-order pipelines with five to six pipeline stages. These processors are often used for less computation-intensive applications, for instance, system control tasks. To support more compute-intensive applications, a key technique to improve performance is branch prediction. Branch prediction has been extensively studies, mostly in the context of application specific custom logic (ASIC) implementations. However, na\"ively porting ASIC-based branch predictors to FPGAs results in slow and/or resource-inefficient implementations since the tradeoffs are different for reconfigurable logic. Wu et al.~\cite{grselect} have shown that a branch predictor design for soft processors should balance its prediction accuracy as well as the maximum operating frequency. They proposed an FPGA-friendly minimalistic branch prediction implementation \textit{gRselect} for Altera's highest performing soft-processor Nios~II-f.

Wu et al. limits the hardware budget of the gRselect predictor to one M9K Block RAM~\cite{StratixIVM9K} on Altera Stratix IV devices, which is the same hardware budget as Nios~II-f. Such a small hardware budget prohibits more elaborated and accurate branch prediction schemes such as perceptron~\cite{perceptron} and TAGE~\cite{tage}. This work looses the hardware budget constraint and investigates FPGA-friendly implementations of perceptron and TAGE predictors. This work also assumes a pipelined processor implementation representative of Altera's Nios~II-f for comparison versus gRselect.

Specifically, this work makes the following contributions: (1)~It studies the FPGA implementation of the perceptron predictor and TAGE predictor, including many optimizations to improve maximum frequency of these predictors. (2)~It shows that comparing the branch direction prediction accuracy over the benchmarks versus gRselect, perceptron is \~1\% worse while TAGE is \~1\% better, assuming these predictors can be accessed in a single cycle. (3)~It discovered that TAGE is too slow for single-cycle access, so in reality TAGE can only provide a prediction in two cycles. This work proposes an overriding predictor that uses a simple base predictor to provide a base prediction in the first cycle, then override that decision should TAGE disagree with the base predictor in the second cycle. It shows that the overriding TAGE predictor achieves 5.2\% better instruction throughput over gRselect.

\section{Background and Goals}
\label{sec:background}

% Di: this section is based on the background section from the grselect paper. I've removed/modified some details not relevant to this paper (BTB and RAS), but the paragraph is mostly the same. I'm leaving it this way for now, probably should rephrase the paragraph later?

Fig.~\ref{fig:bpcanonical} shows the organization of a typical branch predictor comprising a direction predictor and a target predictor.  The predictor operates in the fetch stage where it aims to predict the program counter (PC), that is the address in memory, of the instruction to fetch in the next cycle using the current instruction's PC and other dynamically collected information. The direction predictor guesses whether the branch will be taken or not. The target predictor guesses the address for predicted as taken branches and function returns respectively. The multiplexer at the end selects based on the branch type and the direction prediction whether the target is the fall through address (PC+4 in Nios~II) or the target predicted by the target predictor. Since, at this point in time, the actual instruction is not available in a typical ASIC implementation, it is not directly possible to determine whether the instruction is a return, a branch, or some other instruction. Accordingly, a Selection Logic block uses either pre-decode information or a PC-based, dynamically populated lookup table to guess which target is best to use. With the latter scheme, when no entry exists in the lookup table, some default action is taken until the first time a branch is encountered. Once the branch executes, its type is stored in the lookup table where it serves to identify the branch type on subsequent encounters. This scheme is not perfectly accurate due to aliasing.
\kfig{bpcanonical.pdf}{fig:bpcanonical}{Canonical Branch Predictor.}{angle = 0, trim = 0.2in 1.7in 0.4in 0.1in, clip, width=0.4\textwidth}

\subsection{Design Goals}
\label{sec:background:goal}

This work aims to implement perceptron and TAGE that has high operating frequency as well as accuracy to maximize execution performance. As section~\ref{sec:scheme:tage} will show, a single-cycle TAGE is prohibitively slow. Therefore, this work proposes an overriding TAGE predictor that produces a base prediction in one cycle and overrides that decision with a better prediction if the later prediction differs from the base prediction. Since perceptron and TAGE both requires large storage spaces, on-chip resources is not the main concern of this paper.

\section{Branch Prediction Schemes}
\label{sec:scheme}
This section discusses the structure of the branch predictors considered, including perceptron and TAGE direction predictor and the target predictor. Sections~\ref{sec:scheme:perceptron} and~\ref{sec:scheme:tage} discusses perceptron and tage, while section~\ref{sec:scheme:target} discusses the target predicor.

\subsection{Perceptron Predictor}
\label{sec:scheme:perceptron}
The perceptron predictor use vectors of weights (i.e., perceptrons) to represent correlations among branch instructions~\cite{perceptron}. Fig.~\ref{fig:perceptron} shows the structure of a perceptron predictor. 

When making a prediction, a weight vector is loaded from the table. Then, each weight is multiplied by 1 if the corresponding global branch history is taken, and by -1 otherwise. Finally, the products are summed up, the perceptron predicts taken if the sum is positive, and not taken otherwise.
\kfig{perceptron.pdf}{fig:perceptron}{The preceptron branch predictor.}{angle = 0, trim = 1in 3.5in 2in 1in, clip, width=0.4\textwidth}

\subsection{Tagged Geometric History Length Branch Predictor (TAGE)}
\label{sec:scheme:tage}
The TAGE predictor features a bimodal predictor as base predictor $T_0$ to provide a basic prediction and a set of $M$ tagged predictor components $T_i$~\cite{tage}. These tagged predictor components $T_i$, where $1\leq i\leq M$, are indexed with hash functions of the branch address and the global branch/path history with various length. The global history lengths used for computing the indexing functions for tables $T_i$ form a geometric series, i.e., $L(i) = (int)(\alpha^{i{}^{\_}1}\times L(1)+0.5)$. TAGE achieves its high accuracy by utilizing very long history lengths.

Fig.~\ref{fig:tage} shows a 5-component TAGE predictor. Each table entry has a 3-bit saturating counter \textit{ctr} for prediction result, a \textit{tag}, and a 2-bit useful counter \textit{u}. The indices of the tables are produced by hashing PC and global history with various lengths. A valid prediction result from each table is provided only on a tag match (i.e. a hit). The final prediction of TAGE comes from the hitting tagged predictor component that uses the longest history.
\kfig{tage.pdf}{fig:tage}{A 5-component TAGE branch predictor.}{angle = 0, trim = 0.6in 0.6in 0.4in 0.2in, clip, width=0.4\textwidth}


\subsection{Branch Target Predictor}
\label{sec:scheme:target}
Branch Target Prediction usually requires a Branch Target Buffer (BTB), a cache-like structure that records the addresses of the branches and the target addresses associated with them. If a branch is predicted to be taken and there is also a BTB hit, then the next PC is set to be the predicted target. A BTB can also have set-associativity to reduce the impact from aliasing.

Another common structure for branch target prediction is a Return Address Stack (RAS). It is a stack-like structure that accurately predicts the target address of function returns. When a call instruction executes, the return address of that call is pushed on RAS. When the processor executes the corresponding return instruction, RAS pop the return address and always provides the accurate prediction. Most modern processors have a shallow RAS because typical programs generally do not have very deep call depths. RAS will fail to provide correct target prediction if it is overflowed.

Wu et al. has shown that within the same hardware budget as Nios~II-f (i.e., 1 M9K BRAM), eliminating the BTB and use \textit{Full Address Calculation} (FAC) together with RAS results in better performance~\cite{grselect}. FAC is a technique that calculates the target address in fetch stage to accurately predict target addresses for direct branches, whose target can be calculated based on the instruction itself~\cite{niosii}. Wu et al. has shown that direct branches and returns consist of over 99.8\% of all branches. Implementing FAC with RAS can cover these branches with 100\% accuracy, therefore having a BTB to cover all branches results in negligible improvement in target prediction accuracy. On the other hand, eliminating the BTB and dedicate the entire BRAM for direction prediction improves direction prediction accuracy significantly.

Since we remove the hardware budget constraint in this work, adding a BTB for better target prediction coverage can improve target prediction accuracy. However, our simulations show that the accuracy of the branch predictor is still better without a BTB. This is because when the target predictor only has FAC and RAS, it never predicts indirect branches that are not returns because it is not capable to do so. As a result, the destructive aliasing in the \textbf{direction} predictor is alleviated because less branches are being predicted. Based on this observation, this work uses FAC with RAS as the branch target predictor, which is the same as gRselect.


\section{FPGA Implementation Optimizations}
\label{sec:fpga}
This section discusses FPGA-specific implementation optimizations for perceptron and TAGE. While this section assumes a modern Altera FPGA, the optimizations presented should be broadly applicable.

\subsection{Perceptron Implementation}
\label{sec:fpga:perceptron}

Section~\ref{sec:scheme:perceptron} introduced that perceptron predictor maintains vectors of weights in a table. It produces a prediction through the following steps: (1)~a vector of weight (i.e., a perceptron) is loaded from the table. (2)~multiply the weights with their corresponding global history (1 for taken and -1 for not-taken). (3)~sum up all the products, predict taken if the sum is positive, and not-taken otherwise. Mathematically, for a perceptron predictor using $h$ history bits, each weight vector has $h$ weights $w_{0...h}$, where the bias constant $w_0 = 1$. The predictor has to calculate $y = w_0 + \sum_{i=1}^{h} G_iw_i$, and predict taken if y is positive and not-taken otherwise.

Each of these steps poses difficulties to map to FPGA platform. The rest of this section addresses these problems.

\subsubsection{Perceptron Table Organization}
\label{sec:fpga:perceptron:table}
Each weight in a perceptron is typically 8-bit wide, and perceptron predictors usually use at least 12-bit global history~\cite{perceptron}. The depth of the table, on the other hand, tends to be relatively shallower (e.g. 64 entries for 1KB hardware budget). This requires a very wide but shallow memory, which does not map well to BRAMs on FPGAs. For example, the widest configuration that a M9K BRAM on Altera Stratix IV chips is 36-bit wide times 1k entries~\cite{StratixIVM9K}. If we implement the 1KB perceptron as proposed by Jim\'enez et al.~\cite{perceptron}, which uses 96-bit wide perceptrons with 12-bit global history, it will result in a huge resource inefficiency as shown in Fig~\ref{fig:perceptronTable}. Stratix IV chips have another larger but slower and fewer M144K BRAM on Stratix IV chips~\cite{StratixIVM9K}, which can be configured as wide as 72-bit times 2K entries, clearly the inefficiency problem persists and it would impact maximum operating frequency.

Since typically the perceptron table does not require large storage space, the proposed perceptron implementation uses MLABs as storage, which are fast fine-grain distributed memory resources. Since 50\% of all LABs can be configured as MLAB on Altera Stratix IV devices, using MLABs does not introduce routing difficulty.
\kfig{perceptronTable.pdf}{fig:perceptronTable}{Inefficiency using M9K BRAMs to implement wide but shallow perceptron tables.}{angle = 0, trim = 1in 2in 3.4in 0.5in, clip, width=0.3\textwidth}


\subsubsection{Multiplication}
\label{sec:fpga:perceptron:mult}
The multiplication stage calculates the products of a weights in a perceptron and their global direction histories. Since the value of the global direction history can only be either 1 or -1, the ``multiplication'' degenerates to two cases, i.e., each product can either be the true form or the 2's compliment (i.e., negative) form of each weight. A straight forward implementation is to calculate the negative of each weight and use a mux to select between them based on the corresponding global history, as shown in Fig.~\ref{fig:perceptronMult}(a). To improve maximum frequency, when updating perceptron in execution stage where the branch is resolved, both positive and negative forms of the updated weight can be calculated, and the negatives can be stored a complement perceptron table. In this way, the multiplication stage requires only a 2-to-1 mux, as shown in Fig.~\ref{fig:perceptronMult}(b). As a tradeoff, it requires extra storage for the negative weights.
\kfig{perceptronMult.pdf}{fig:perceptronMult}{Perceptron multiplication implementation.}{angle = 0, trim = 0.3in 2in 3in 0.6in, clip, width=0.4\textwidth}

\subsubsection{Adder Tree}
\label{sec:fpga:perceptron:adder}
The adder tree sums the products from the multiplication stage. As Section~\ref{sec:eval:ipc} will show, at least 16 global histories has to be used to achieve sufficient accuracy. Implementing a 16-to-1 adder tree for 8-bit integers na\''ively degrades maximum frequency severally. The maximum frequency has to be improved for perceptron to be a practical implementaion.

This work employs \textit{Low Order Bit (LOB) Elimination} proposed by Aasaraai et al.~\cite{lob}. The idea is to ignore the Low Order Bits (LOBs) of each weight and only use the High Order Bits (HOBs) during prediction, but use all the bits during update. Section~\ref{sec:eval:ipc} shows that eliminating 5 LOB bits is only (TODO: getting this data soon)x?\% less accurate than using all 8 bits, but summing fewer bits results in significantly higher maximum frequency. Our experiments have shown that using 3 HOB for prediction achieves the best balanced performance.

Cadenas et al.~\cite{perceptronRearrange} proposed a method to rearrange the weights stored in the table in order to reduce the number of layers of the adder tree. Assuming a perceptron predictor uses $h$ history bits, in stead of storing $h$ weights $w_i$ where $i = 1 ... h$, a new form of weights $\widetilde{w}_i$: $\widetilde{w}_i = - w_i + w_{i+1};, \widetilde{w}_{i+1} = - w_i - w_{i+1},$ for $i = 1, 3, ..., h-1$. The perceptron prediction can now be computed by $y = w_0 + \sum_{i=1}^{h/2}(-G_{2j-1})\widetilde{w}_{2j+G_{2j}G_{2j}}$. The new arrangement pushes part of the calculation to the less time critical update logic of the perceptron predictor so that only $h/2$ additions have to be performed, hence reduces the number of adder required by 50\%.

The adder tree implementation of this work uses LOB elimination and the new arrangement of weights. To further improve maximum frequency, the adder tree is also hand optimized at the granularity of \textit{Full Adder} (FA) and \textit{Half Adder} (HA), and it is proven to be 10\% faster than the logic synthesized by the CAD tool with straight forward Verilog code.


\subsection{TAGE Implementation}
\label{sec:fpga:tage}
Section~\ref{sec:eval:ipc} shows that TAGE predictor is the most accurate amongst all the direction predictors considered in this work with the same hardware budget. However, TAGE uses multiple tables with tagged entries that require comparator driven logic, which does not map well onto FPGAs. Section~\ref{sec:eval:ips} shows that the maximum frequency slowdown of TAGE cannot be justified by the accuracy gain.

The critical path of TAGE is as follows: it performs a complicated PC based hashing to generate the indices to the tables, compares the tags of the loaded entries to determine whether they are hits or not, finally each decision has to fall through cascaded layers of multiplexers. Although the latency to performing these operations is high, the path can be easily pipelined to achieve much higher operating frequency. Based on this observation, this work explores an overriding branch predictor implementation using TAGE. Overriding branch prediction is a technique to leverage the benefits between fast predictors and accurate predictors. This technique has been used on the Alpha EV8~\cite{alphaEV8} microprocessors. In an overriding predictor, a faster but less accurate base predictor makes a base prediction quickly, and then a slower but more accurate predictor overrides that decision if it disagrees with the base prediction. 

In this work, the base predictor is the simple bimodal predictor included in TAGE it self, i.e., $T_0$ in Fig.~\ref{fig:tage}. The bimodal predictor provides a base prediction in the first cycle, and TAGE provides a prediction at the second cycle. Section~\ref{sec:eval:ipc} and~\ref{sec:eval:fmax} show that the overriding TAGE outperforms all the other branch prediction schemes in terms of both accuracy and maximum frequency.

\section{Evaluation}
\label{sec:eval}
This section presents the evaluation of the branch predictors considered in this work. Section~\ref{sec:eval:methodology} details the experimental
methodology. Section~\ref{sec:eval:ipc} compares the accuracy of various direction predictors including bimiodal, gshare, grselect, perceptron and TAGE. It shows that the overriding TAGE is the most accurate. Section~\ref{sec:eval:fmax} reports maximum operating frequency as well as FPGA resource usage. Finally, Section~\ref{sec:eval:ips} reports the overall performance, showing that the overriding TAGE predictor is the best performing predictor.

As discussed in section~\ref{sec:scheme:target}, all the evaluation results presented in this section assumes the same target prediction scheme, which includes a FAC and RAS, the same target predictor used in the gRselect predictor~\cite{grselect}.

\subsection{Methodology}
\label{sec:eval:methodology}
% Di: mostly based on the grselect paper, rephrase?
To compare the predictors this work measures: (1) Accuracy as Instruction Per Cycle (IPC), a frequency agnostic metric that isolates the effects of implementation, (2) Instructions Per Second (IPS), a true measure of performance, (4) Operating frequency, and (5) resource usage. Simulation measures IPC using a custom, cycle-accurate, full-system Nios~II simulator. The simulator boots ucLinux~\cite{uclinux}, and runs a subset of SPEC CPU2006 integer benchmarks with reference inputs~\cite{spec2k6}.

The predictors used as comparison includes bimodal, gshare and grselect. These predictors are implemented faithfully with the same techniques presented by Wu et al.~\cite{grselect}. All designs were implemented in Verilog and synthesized using Quartus II 13.0 on a Stratix IV EP4SE230F29C2 chip in order to measure their maximum clock frequency and area cost. The maximum frequency is reported as the average maximum clock frequency of five placement and routing passes with different random seeds. Area usage is reported in terms of ALUTs used.


\subsection{Branch Prediction Accuracy}
\label{sec:eval:ipc}
This section first presents data that justify the final design of perceptron and TAGE configurations, then a comparison versus bimodal, gshare and grselect is presented.

\subsubsection{Perceptron}
\label{sec:eval:ipc:perceptron}
This work experiments perceptron predictor with hardware budget ranging from 1KB to 32KB. For each of the hardware budget, this work also experiments different configuration with various number of global history bits used.

Fig.~\ref{fig:perceptronIPC} shows the best performing perceptron configuration for each hardware budget. 



\subsubsection{TAGE}
\label{sec:eval:ipc:tage}



\subsubsection{Accuracy Comparison}
Range from 1KB-32KB
Perceptron doesn't work well
TAGE overriding with statistic corrector



Perceptron HOB accuracy
% Di: I'll get these data soon...

\subsection{Area}
\label{sec:eval:area}

%include memory bits? Perceptron does not use BRAMs, but MLABs


\section{Conclusion}
The conclusion goes here.




% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,bp}





% that's all folks
\end{document}


